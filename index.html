<!DOCTYPE html>
<html>
<head>
  <title>NLP 101</title>
  <meta charset="utf-8">
  <meta name="description" content="NLP 101">
  <meta name="author" content="lusy (vaseva@mi.fu-berlin.de)">
  <meta name="generator" content="slidify" />
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta http-equiv="X-UA-Compatible" content="chrome=1">
  <link rel="stylesheet" href="libraries/frameworks/io2012/css/default.css" media="all" >
  <link rel="stylesheet" href="libraries/frameworks/io2012/css/phone.css" 
    media="only screen and (max-device-width: 480px)" >
  <link rel="stylesheet" href="libraries/frameworks/io2012/css/slidify.css" >
  <link rel="stylesheet" href="libraries/highlighters/highlight.js/css/tomorrow.css" />
  <base target="_blank"> <!-- This amazingness opens all links in a new tab. -->  <link rel=stylesheet href="./assets/css/ribbons.css"></link>
<link rel=stylesheet href="./assets/css/userdefined.css"></link>

  
  <!-- Grab CDN jQuery, fall back to local if offline -->
  <script src="http://ajax.aspnetcdn.com/ajax/jQuery/jquery-1.7.min.js"></script>
  <script>window.jQuery || document.write('<script src="libraries/widgets/quiz/js/jquery.js"><\/script>')</script> 
  <script data-main="libraries/frameworks/io2012/js/slides" 
    src="libraries/frameworks/io2012/js/require-1.0.8.min.js">
  </script>
  
  

</head>
<body style="opacity: 0">
  <slides class="layout-widescreen">
    
    <!-- LOGO SLIDE -->
        <slide class="title-slide segue nobackground">
  <hgroup class="auto-fadein">
    <h1>NLP 101</h1>
    <h2>aided by python and nltk</h2>
    <p>lusy (vaseva@mi.fu-berlin.de)<br/></p>
  </hgroup>
  <article></article>  
</slide>
    

    <!-- SLIDES -->
    <slide class="" id="slide-1" style="background:;">
  <article data-timings="">
    
<div style='position:absolute;bottom:50%;left:30%;font-size:30px'>
  <p><a href="https://lusy.github.io/nltk-talk-bcn/">https://lusy.github.io/nltk-talk-bcn/</a></p>

</div>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-2" style="background:;">
  <hgroup>
    <h2>NLP / NLTK</h2>
  </hgroup>
  <article data-timings="">
    <h3>NLP</h3>

<ul>
<li>intersection of computer science and linguistics</li>
<li>computational models for natural language</li>
<li>small scale: chunking, word forms</li>
<li>big scale: parser, classifier, recommender</li>
</ul>

<h3>NLTK</h3>

<ul>
<li>suite of libs for nlp</li>
<li>University of Pennsylvania 2001</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-3" style="background:;">
  <hgroup>
    <h2>Reload Jane Austen</h2>
  </hgroup>
  <article data-timings="">
    <pre><code class="bash">&gt; switch to console
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-4" style="background:;">
  <hgroup>
    <h2>Take a step back</h2>
  </hgroup>
  <article data-timings="">
    <pre><code class="python">def generate(self, length=100):
    &quot;&quot;&quot;
    Print random text, generated using a trigram language model.

    :param length: The length of text to generate (default=100)
    :type length: int
    :seealso: NgramModel
    &quot;&quot;&quot;
    if &#39;_trigram_model&#39; not in self.__dict__:
        print &quot;Building ngram index...&quot;
        estimator = lambda fdist, bins: LidstoneProbDist(fdist, 0.2)
        self._trigram_model = NgramModel(3, self, estimator=estimator)
    text = self._trigram_model.generate(length)
    print tokenwrap(text)
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-5" style="background:;">
  <hgroup>
    <h2>Ngram?</h2>
  </hgroup>
  <article data-timings="">
    <ul class = "build incremental">
<li>a sequence of n consecutive words (tokens)</li>
<li><em>I like fluffy ponies.</em>

<ul>
<li>bigrams: &quot;I like&quot;, &quot;like fluffy&quot;, &quot;fluffy ponies&quot;</li>
<li>trigrams: &quot;I like fluffy&quot;, &quot;like fluffy ponies&quot;</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-6" style="background:;">
  <hgroup>
    <h2>What is a (probabilistic) language model?</h2>
  </hgroup>
  <article data-timings="">
    <h3>Model that computes the probabilities for:</h3>

<ul>
<li>a sentence to appear \(P(W)\)</li>
<li>an upcoming word of a sentence \(P(w_n|w_1,w_2,...,w_{n-1})\)</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-7" style="background:;">
  <hgroup>
    <h2>Some maths</h2>
  </hgroup>
  <article data-timings="">
    <h3>chain probability rule</h3>

<p>\[ P(A|B) = \frac{P(A,B)}{P(B)} \Rightarrow  P(A,B) = P(A|B) \times P(B) \]</p>

<p>generalize:
\[ P(x_1,x_2,x_3,\dotsc,x_n) = P(x_1)P(x_2|x_1)P(x_3|x_1,x_2) \dotsm P(x_n|x_1,...,x_{n-1}) \]</p>

<p>so:
\[ P(\text{"I like fluffy ponies"}) = P(\text{I}) \times P(\text{like}|\text{I}) \times P(\text{fluffy}|\text{I like}) \times P(\text{ponies}|\text{I like fluffy}) \]</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-8" style="background:;">
  <hgroup>
    <h2>How do we compute the probabilities?</h2>
  </hgroup>
  <article data-timings="">
    <h3>Just count occurancies?</h3>

<p>\[
\begin{align*}
P(\text{ponies}|\text{I like fluffy}) = \\
\\
\frac{\text{count}(\text{I like fluffy ponies})}{\text{count}(\text{I like fluffy})}
\end{align*}
\\
\]</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-9" style="background:;">
  <hgroup>
    <h2>Markov assupmtion: simplify!</h2>
  </hgroup>
  <article data-timings="">
    <blockquote>
<p>the probability for the upcoming word given the entire context would be similar to the probability for it given just the last couple of words</p>
</blockquote>

<p>\[
P(\text{rainbows}|\text{I like pink fluffy ponies dancing on}) \approx P(\text{rainbows}|\text{on})
\]</p>

<p>or</p>

<p>\[
P(\text{rainbows}|\text{I like pink fluffy ponies dancing on}) \approx P(\text{rainbows}|\text{dancing on})
\]</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-10" style="background:;">
  <hgroup>
    <h2>Simplest case: Unigram model</h2>
  </hgroup>
  <article data-timings="">
    <p>\[
P(w_1,w_2,\dotsc,w_n) \approx P(w_1)P(w_2) \dotsm P(w_n)
\]</p>

<p>in other words:</p>

<p>\[
P(\text{rainbows}|\text{I like pink fluffy ponies dancing on}) \approx P(\text{rainbows})
\]</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-11" style="background:;">
  <hgroup>
    <h2>Ngram models</h2>
  </hgroup>
  <article data-timings="">
    <h3>bigram model</h3>

<p>conditions on the previous word:</p>

<p>\[ P_{MLE}(w_i|w_{i-1}) = \frac{\text{count}(w_{i-1},w_i)}{\text{count}(w_{i-1})}\]</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-12" style="background:;">
  <hgroup>
    <h2>Ngram models</h2>
  </hgroup>
  <article data-timings="">
    <h3>trigram model</h3>

<p>conditions on the sequence of the previous 2 words:</p>

<p>\[
P(w_i|w_{i-2},w_{i-1}) = \frac{\text{count}(w_{i-2},w_{i-1},w_i)}{\text{count}(w_{i-2},w_{i-1})}
\]</p>

<p>\[
\begin{multline}
P(\text{*s* I like pink fluffy unicorns *e*}) = \\
P(\text{I}|\text{*s*})P(\text{like}|\text{*s* I})P(\text{pink}|\text{I like})P(\text{fluffy}|\text{like pink})\\
P(\text{unicorns}|\text{pink fluffy})P(\text{*e*}|\text{fluffy unicorns})
\end{multline}
\]</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-13" style="background:;">
  <hgroup>
    <h2>Language Models</h2>
  </hgroup>
  <article data-timings="">
    <ul>
<li>So we train the chosen model on a training set.</li>
<li>However, if we use the model in its &quot;pure&quot; form, no unseen ngrams could be generated (predicted)!</li>
<li>And the best language model is one that best predicts a (unseen) test set.</li>
</ul>

<p>\(\rightarrow\) generalization: that&#39;s what we need the estimator for (and for smoothing)</p>

<pre><code class="python">def generate(self, length=100):
    if &#39;_trigram_model&#39; not in self.__dict__:
        print &quot;Building ngram index...&quot;
        estimator = lambda fdist, bins: LidstoneProbDist(fdist, 0.2)
        self._trigram_model = NgramModel(3, self, estimator=estimator)
    text = self._trigram_model.generate(length)
    print tokenwrap(text)
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-14" style="background:;">
  <hgroup>
    <h2>Generalization</h2>
  </hgroup>
  <article data-timings="">
    <h3>Intuition</h3>

<p>$ P(\text{dinosaurs}|\text{pink fluffy}) = 0 $ (doesn&#39;t appear in the training set)</p>

<p>\(\rightarrow\) so we have no chance to predict it</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-15" style="background:;">
  <hgroup>
    <h2>Generalization: Add 1/Laplace smoothing</h2>
  </hgroup>
  <article data-timings="">
    <h3>Intuition: add 1 to all the counts in the training set</h3>

<table><thead>
<tr>
<th></th>
<th align="center">I</th>
<th align="center">like</th>
<th align="center">pink</th>
<th align="center">fluffy</th>
<th align="center">unicorns</th>
</tr>
</thead><tbody>
<tr>
<td>I</td>
<td align="center">0</td>
<td align="center">23</td>
<td align="center">0</td>
<td align="center">0</td>
<td align="center">0</td>
</tr>
<tr>
<td>like</td>
<td align="center">25</td>
<td align="center">4</td>
<td align="center">35</td>
<td align="center">0</td>
<td align="center">0</td>
</tr>
<tr>
<td>pink</td>
<td align="center">0</td>
<td align="center">8</td>
<td align="center">0</td>
<td align="center">5</td>
<td align="center">3</td>
</tr>
<tr>
<td>fluffy</td>
<td align="center">0</td>
<td align="center">2</td>
<td align="center">0</td>
<td align="center">0</td>
<td align="center">6</td>
</tr>
<tr>
<td>unicorns</td>
<td align="center">3</td>
<td align="center">1</td>
<td align="center">0</td>
<td align="center">0</td>
<td align="center">0</td>
</tr>
</tbody></table>

<ul class = "build incremental">
<li>Not really used with ngram models.

<ul>
<li>Changes probabilities massively!</li>
</ul></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-16" style="background:;">
  <hgroup>
    <h2>Generalization: Backoff</h2>
  </hgroup>
  <article data-timings="">
    <h3>Intuition:</h3>

<p>use less context for unknown stuff.</p>

<p>So if we have good evidence we use trigrams, otherwise bigrams, otherwise unigrams.</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-17" style="background:;">
  <hgroup>
    <h2>Taking another look...</h2>
  </hgroup>
  <article data-timings="">
    <pre><code class="python">def generate(self, length=100):
    &quot;&quot;&quot;
    Print random text, generated using a trigram language model.

    :param length: The length of text to generate (default=100)
    :type length: int
    :seealso: NgramModel
    &quot;&quot;&quot;
    if &#39;_trigram_model&#39; not in self.__dict__:
        print &quot;Building ngram index...&quot;
        estimator = lambda fdist, bins: LidstoneProbDist(fdist, 0.2)
        self._trigram_model = NgramModel(3, self, estimator=estimator)
    text = self._trigram_model.generate(length)
    print tokenwrap(text)
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-18" style="background:;">
  <hgroup>
    <h2>So what&#39;s next?</h2>
  </hgroup>
  <article data-timings="">
    <pre><code class="python">&gt;&gt;&gt; import nltk
&gt;&gt;&gt; nltk.chat.chatbots()
</code></pre>

<pre><code>Which chatbot would you like to talk to?
1: Eliza (psycho-babble)
2: Iesha (teen anime junky)
3: Rude (abusive bot)
4: Suntsu (Chinese sayings)
5: Zen (gems of wisdom)

Enter a number in the range 1-5: 1
========================================================================
Hello.  How are you feeling today?
&gt;
</code></pre>

<p>It&#39;s your turn ;)</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="" id="slide-19" style="background:;">
  <hgroup>
    <h2>Thanks!</h2>
  </hgroup>
  <article data-timings="">
    <h3>a.k.a. references</h3>

<ul>
<li><a href="http://www.nltk.org/book/">NLTK Book</a></li>
<li class='..'>and NLTK source code ;)</li>
<li>Stanford Coursera class on NLP: Dan Jurafsky + Christopher Manning</li>
<li><a href="http://slidify.org/">slidify</a> for building these slides</li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

    <slide class="backdrop"></slide>
  </slides>
  <div class="pagination pagination-small" id='io2012-ptoc' style="display:none;">
    <ul>
      <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=1 title=''>
         1
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=2 title='NLP / NLTK'>
         2
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=3 title='Reload Jane Austen'>
         3
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=4 title='Take a step back'>
         4
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=5 title='Ngram?'>
         5
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=6 title='What is a (probabilistic) language model?'>
         6
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=7 title='Some maths'>
         7
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=8 title='How do we compute the probabilities?'>
         8
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=9 title='Markov assupmtion: simplify!'>
         9
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=10 title='Simplest case: Unigram model'>
         10
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=11 title='Ngram models'>
         11
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=12 title='Ngram models'>
         12
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=13 title='Language Models'>
         13
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=14 title='Generalization'>
         14
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=15 title='Generalization: Add 1/Laplace smoothing'>
         15
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=16 title='Generalization: Backoff'>
         16
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=17 title='Taking another look...'>
         17
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=18 title='So what&#39;s next?'>
         18
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=19 title='Thanks!'>
         19
      </a>
    </li>
  </ul>
  </div>  <!--[if IE]>
    <script 
      src="http://ajax.googleapis.com/ajax/libs/chrome-frame/1/CFInstall.min.js">  
    </script>
    <script>CFInstall.check({mode: 'overlay'});</script>
  <![endif]-->
</body>
  <!-- Load Javascripts for Widgets -->
  
  <!-- MathJax: Fall back to local if CDN offline but local image fonts are not supported (saves >100MB) -->
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        processEscapes: true
      }
    });
  </script>
  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/2.0-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  <!-- <script src="https://c328740.ssl.cf1.rackcdn.com/mathjax/2.0-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script> -->
  <script>window.MathJax || document.write('<script type="text/x-mathjax-config">MathJax.Hub.Config({"HTML-CSS":{imageFont:null}});<\/script><script src="libraries/widgets/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"><\/script>')
</script>
<!-- LOAD HIGHLIGHTER JS FILES -->
  <script src="libraries/highlighters/highlight.js/highlight.pack.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
  <!-- DONE LOADING HIGHLIGHTER JS FILES -->
   
  </html>